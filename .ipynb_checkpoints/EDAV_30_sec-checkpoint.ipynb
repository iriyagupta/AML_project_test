{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "210bb436",
   "metadata": {
    "id": "210bb436"
   },
   "source": [
    "## Project Team 8 - MGR : Music Genre Classification and Recoginition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524a65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import important modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e27c90a6",
   "metadata": {
    "id": "e27c90a6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAE3RFWHRUaXRsZQB2bGFnIGNvbG9ybWFwQ/+bDQAAABl0RVh0RGVzY3JpcHRpb24AdmxhZyBjb2xvcm1hcMhNC5UAAAAwdEVYdEF1dGhvcgBNYXRwbG90bGliIHYzLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZ/e3zs8AAAAydEVYdFNvZnR3YXJlAE1hdHBsb3RsaWIgdjMuNS4xLCBodHRwczovL21hdHBsb3RsaWIub3Jn2RFR6AAAAhhJREFUeJzt1kFuqzAYhVGbLqWre3vs2qATnArDX0j1ZvecSYRjbEOj6uuf/7621lr76K1dfS6n6+14vVTz/tdn//X7Xt239Efz+u3619/X49O5l/fWOd1fzjuuP+bPz3net1+O96Wa9/v1eM+388b46bzFvqe/33Gd87mnfcr1qvc4n6f6+83PU41Xzz+f87h//d6v3/cY720e3z/b8Xr2Gt624xfjevrc5ut1nb5f9/Ex73g95o95bb2+72fdafx1Xd0/1t/3X6/vb9O8cr/5+ar15+d+un7xHqr3Ut9fnWva/+G6fz7n3f6v6/f2eX7Op/vP5336vt97vtvfefk7fXbu+vd2c659fP83BQAkEQAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAECgb31CUaaV0kkSAAAAAElFTkSuQmCC\n",
      "text/html": [
       "<div style=\"vertical-align: middle;\"><strong>vlag</strong> </div><div class=\"cmap\"><img alt=\"vlag colormap\" title=\"vlag\" style=\"border: 1px solid #555;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAE3RFWHRUaXRsZQB2bGFnIGNvbG9ybWFwQ/+bDQAAABl0RVh0RGVzY3JpcHRpb24AdmxhZyBjb2xvcm1hcMhNC5UAAAAwdEVYdEF1dGhvcgBNYXRwbG90bGliIHYzLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZ/e3zs8AAAAydEVYdFNvZnR3YXJlAE1hdHBsb3RsaWIgdjMuNS4xLCBodHRwczovL21hdHBsb3RsaWIub3Jn2RFR6AAAAhhJREFUeJzt1kFuqzAYhVGbLqWre3vs2qATnArDX0j1ZvecSYRjbEOj6uuf/7621lr76K1dfS6n6+14vVTz/tdn//X7Xt239Efz+u3619/X49O5l/fWOd1fzjuuP+bPz3net1+O96Wa9/v1eM+388b46bzFvqe/33Gd87mnfcr1qvc4n6f6+83PU41Xzz+f87h//d6v3/cY720e3z/b8Xr2Gt624xfjevrc5ut1nb5f9/Ex73g95o95bb2+72fdafx1Xd0/1t/3X6/vb9O8cr/5+ar15+d+un7xHqr3Ut9fnWva/+G6fz7n3f6v6/f2eX7Op/vP5336vt97vtvfefk7fXbu+vd2c659fP83BQAkEQAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAEAgAQAAgQQAAAQSAAAQSAAAQCABAACBBAAABBIAABBIAABAIAEAAIEEAAAEEgAAEEgAAECgb31CUaaV0kkSAAAAAElFTkSuQmCC\"></div><div style=\"vertical-align: middle; max-width: 514px; display: flex; justify-content: space-between;\"><div style=\"float: left;\"><div title=\"#2369bdff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #2369bdff;\"></div> under</div><div style=\"margin: 0 auto; display: inline-block;\">bad <div title=\"#00000000\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #00000000;\"></div></div><div style=\"float: right;\">over <div title=\"#a9373bff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #a9373bff;\"></div></div>"
      ],
      "text/plain": [
       "<matplotlib.colors.ListedColormap at 0x14b1ecd60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    \n",
    "    \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.color_palette(\"vlag\", as_cmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360aa61c-7ed4-40fc-be4d-4153a2eca305",
   "metadata": {},
   "source": [
    "#### Dataset and information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07ded7a8-6262-4010-b842-35a2c3315919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>chroma_stft_mean</th>\n",
       "      <th>chroma_stft_var</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_centroid_var</th>\n",
       "      <th>spectral_bandwidth_mean</th>\n",
       "      <th>spectral_bandwidth_var</th>\n",
       "      <th>rolloff_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc16_var</th>\n",
       "      <th>mfcc17_mean</th>\n",
       "      <th>mfcc17_var</th>\n",
       "      <th>mfcc18_mean</th>\n",
       "      <th>mfcc18_var</th>\n",
       "      <th>mfcc19_mean</th>\n",
       "      <th>mfcc19_var</th>\n",
       "      <th>mfcc20_mean</th>\n",
       "      <th>mfcc20_var</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blues.00000.wav</th>\n",
       "      <td>661794</td>\n",
       "      <td>0.350088</td>\n",
       "      <td>0.088757</td>\n",
       "      <td>0.130228</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>1784.165850</td>\n",
       "      <td>129774.064525</td>\n",
       "      <td>2002.449060</td>\n",
       "      <td>85882.761315</td>\n",
       "      <td>3805.839606</td>\n",
       "      <td>...</td>\n",
       "      <td>52.420910</td>\n",
       "      <td>-1.690215</td>\n",
       "      <td>36.524071</td>\n",
       "      <td>-0.408979</td>\n",
       "      <td>41.597103</td>\n",
       "      <td>-2.303523</td>\n",
       "      <td>55.062923</td>\n",
       "      <td>1.221291</td>\n",
       "      <td>46.936035</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blues.00001.wav</th>\n",
       "      <td>661794</td>\n",
       "      <td>0.340914</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.095948</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>1530.176679</td>\n",
       "      <td>375850.073649</td>\n",
       "      <td>2039.036516</td>\n",
       "      <td>213843.755497</td>\n",
       "      <td>3550.522098</td>\n",
       "      <td>...</td>\n",
       "      <td>55.356403</td>\n",
       "      <td>-0.731125</td>\n",
       "      <td>60.314529</td>\n",
       "      <td>0.295073</td>\n",
       "      <td>48.120598</td>\n",
       "      <td>-0.283518</td>\n",
       "      <td>51.106190</td>\n",
       "      <td>0.531217</td>\n",
       "      <td>45.786282</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blues.00002.wav</th>\n",
       "      <td>661794</td>\n",
       "      <td>0.363637</td>\n",
       "      <td>0.085275</td>\n",
       "      <td>0.175570</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>1552.811865</td>\n",
       "      <td>156467.643368</td>\n",
       "      <td>1747.702312</td>\n",
       "      <td>76254.192257</td>\n",
       "      <td>3042.260232</td>\n",
       "      <td>...</td>\n",
       "      <td>40.598766</td>\n",
       "      <td>-7.729093</td>\n",
       "      <td>47.639427</td>\n",
       "      <td>-1.816407</td>\n",
       "      <td>52.382141</td>\n",
       "      <td>-3.439720</td>\n",
       "      <td>46.639660</td>\n",
       "      <td>-2.231258</td>\n",
       "      <td>30.573025</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blues.00003.wav</th>\n",
       "      <td>661794</td>\n",
       "      <td>0.404785</td>\n",
       "      <td>0.093999</td>\n",
       "      <td>0.141093</td>\n",
       "      <td>0.006346</td>\n",
       "      <td>1070.106615</td>\n",
       "      <td>184355.942417</td>\n",
       "      <td>1596.412872</td>\n",
       "      <td>166441.494769</td>\n",
       "      <td>2184.745799</td>\n",
       "      <td>...</td>\n",
       "      <td>44.427753</td>\n",
       "      <td>-3.319597</td>\n",
       "      <td>50.206673</td>\n",
       "      <td>0.636965</td>\n",
       "      <td>37.319130</td>\n",
       "      <td>-0.619121</td>\n",
       "      <td>37.259739</td>\n",
       "      <td>-3.407448</td>\n",
       "      <td>31.949339</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blues.00004.wav</th>\n",
       "      <td>661794</td>\n",
       "      <td>0.308526</td>\n",
       "      <td>0.087841</td>\n",
       "      <td>0.091529</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>1835.004266</td>\n",
       "      <td>343399.939274</td>\n",
       "      <td>1748.172116</td>\n",
       "      <td>88445.209036</td>\n",
       "      <td>3579.757627</td>\n",
       "      <td>...</td>\n",
       "      <td>86.099236</td>\n",
       "      <td>-5.454034</td>\n",
       "      <td>75.269707</td>\n",
       "      <td>-0.916874</td>\n",
       "      <td>53.613918</td>\n",
       "      <td>-4.404827</td>\n",
       "      <td>62.910812</td>\n",
       "      <td>-11.703234</td>\n",
       "      <td>55.195160</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 length  chroma_stft_mean  chroma_stft_var  rms_mean  \\\n",
       "filename                                                               \n",
       "blues.00000.wav  661794          0.350088         0.088757  0.130228   \n",
       "blues.00001.wav  661794          0.340914         0.094980  0.095948   \n",
       "blues.00002.wav  661794          0.363637         0.085275  0.175570   \n",
       "blues.00003.wav  661794          0.404785         0.093999  0.141093   \n",
       "blues.00004.wav  661794          0.308526         0.087841  0.091529   \n",
       "\n",
       "                  rms_var  spectral_centroid_mean  spectral_centroid_var  \\\n",
       "filename                                                                   \n",
       "blues.00000.wav  0.002827             1784.165850          129774.064525   \n",
       "blues.00001.wav  0.002373             1530.176679          375850.073649   \n",
       "blues.00002.wav  0.002746             1552.811865          156467.643368   \n",
       "blues.00003.wav  0.006346             1070.106615          184355.942417   \n",
       "blues.00004.wav  0.002303             1835.004266          343399.939274   \n",
       "\n",
       "                 spectral_bandwidth_mean  spectral_bandwidth_var  \\\n",
       "filename                                                           \n",
       "blues.00000.wav              2002.449060            85882.761315   \n",
       "blues.00001.wav              2039.036516           213843.755497   \n",
       "blues.00002.wav              1747.702312            76254.192257   \n",
       "blues.00003.wav              1596.412872           166441.494769   \n",
       "blues.00004.wav              1748.172116            88445.209036   \n",
       "\n",
       "                 rolloff_mean  ...  mfcc16_var  mfcc17_mean  mfcc17_var  \\\n",
       "filename                       ...                                        \n",
       "blues.00000.wav   3805.839606  ...   52.420910    -1.690215   36.524071   \n",
       "blues.00001.wav   3550.522098  ...   55.356403    -0.731125   60.314529   \n",
       "blues.00002.wav   3042.260232  ...   40.598766    -7.729093   47.639427   \n",
       "blues.00003.wav   2184.745799  ...   44.427753    -3.319597   50.206673   \n",
       "blues.00004.wav   3579.757627  ...   86.099236    -5.454034   75.269707   \n",
       "\n",
       "                 mfcc18_mean  mfcc18_var  mfcc19_mean  mfcc19_var  \\\n",
       "filename                                                            \n",
       "blues.00000.wav    -0.408979   41.597103    -2.303523   55.062923   \n",
       "blues.00001.wav     0.295073   48.120598    -0.283518   51.106190   \n",
       "blues.00002.wav    -1.816407   52.382141    -3.439720   46.639660   \n",
       "blues.00003.wav     0.636965   37.319130    -0.619121   37.259739   \n",
       "blues.00004.wav    -0.916874   53.613918    -4.404827   62.910812   \n",
       "\n",
       "                 mfcc20_mean  mfcc20_var  label  \n",
       "filename                                         \n",
       "blues.00000.wav     1.221291   46.936035  blues  \n",
       "blues.00001.wav     0.531217   45.786282  blues  \n",
       "blues.00002.wav    -2.231258   30.573025  blues  \n",
       "blues.00003.wav    -3.407448   31.949339  blues  \n",
       "blues.00004.wav   -11.703234   55.195160  blues  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/features_30_sec.csv')\n",
    "df = df.set_index('filename')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e7b645d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>chroma_stft_mean</th>\n",
       "      <th>chroma_stft_var</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_centroid_var</th>\n",
       "      <th>spectral_bandwidth_mean</th>\n",
       "      <th>spectral_bandwidth_var</th>\n",
       "      <th>rolloff_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc16_mean</th>\n",
       "      <th>mfcc16_var</th>\n",
       "      <th>mfcc17_mean</th>\n",
       "      <th>mfcc17_var</th>\n",
       "      <th>mfcc18_mean</th>\n",
       "      <th>mfcc18_var</th>\n",
       "      <th>mfcc19_mean</th>\n",
       "      <th>mfcc19_var</th>\n",
       "      <th>mfcc20_mean</th>\n",
       "      <th>mfcc20_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>662030.846000</td>\n",
       "      <td>0.378682</td>\n",
       "      <td>0.086340</td>\n",
       "      <td>0.130930</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>2201.780898</td>\n",
       "      <td>4.696916e+05</td>\n",
       "      <td>2242.541070</td>\n",
       "      <td>137079.155165</td>\n",
       "      <td>4571.549304</td>\n",
       "      <td>...</td>\n",
       "      <td>1.148144</td>\n",
       "      <td>60.730958</td>\n",
       "      <td>-3.966028</td>\n",
       "      <td>62.633624</td>\n",
       "      <td>0.507696</td>\n",
       "      <td>63.712586</td>\n",
       "      <td>-2.328761</td>\n",
       "      <td>66.231930</td>\n",
       "      <td>-1.095348</td>\n",
       "      <td>70.126096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1784.073992</td>\n",
       "      <td>0.081705</td>\n",
       "      <td>0.007735</td>\n",
       "      <td>0.065683</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>715.960600</td>\n",
       "      <td>4.008995e+05</td>\n",
       "      <td>526.316473</td>\n",
       "      <td>96455.666326</td>\n",
       "      <td>1574.791602</td>\n",
       "      <td>...</td>\n",
       "      <td>4.578948</td>\n",
       "      <td>33.781951</td>\n",
       "      <td>4.549697</td>\n",
       "      <td>33.479172</td>\n",
       "      <td>3.869105</td>\n",
       "      <td>34.401977</td>\n",
       "      <td>3.755957</td>\n",
       "      <td>37.174631</td>\n",
       "      <td>3.837007</td>\n",
       "      <td>45.228512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>660000.000000</td>\n",
       "      <td>0.171939</td>\n",
       "      <td>0.044555</td>\n",
       "      <td>0.005276</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>570.040355</td>\n",
       "      <td>7.911251e+03</td>\n",
       "      <td>898.066208</td>\n",
       "      <td>10787.185064</td>\n",
       "      <td>749.140636</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.693844</td>\n",
       "      <td>9.169314</td>\n",
       "      <td>-17.234728</td>\n",
       "      <td>13.931521</td>\n",
       "      <td>-11.963694</td>\n",
       "      <td>15.420555</td>\n",
       "      <td>-18.501955</td>\n",
       "      <td>13.487622</td>\n",
       "      <td>-19.929634</td>\n",
       "      <td>7.956583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>661504.000000</td>\n",
       "      <td>0.319562</td>\n",
       "      <td>0.082298</td>\n",
       "      <td>0.086657</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>1627.697311</td>\n",
       "      <td>1.843505e+05</td>\n",
       "      <td>1907.240605</td>\n",
       "      <td>67376.554428</td>\n",
       "      <td>3380.069642</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.863280</td>\n",
       "      <td>40.376442</td>\n",
       "      <td>-7.207225</td>\n",
       "      <td>40.830875</td>\n",
       "      <td>-2.007015</td>\n",
       "      <td>41.884240</td>\n",
       "      <td>-4.662925</td>\n",
       "      <td>41.710184</td>\n",
       "      <td>-3.368996</td>\n",
       "      <td>42.372865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>661794.000000</td>\n",
       "      <td>0.383148</td>\n",
       "      <td>0.086615</td>\n",
       "      <td>0.122443</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>2209.263090</td>\n",
       "      <td>3.384862e+05</td>\n",
       "      <td>2221.392843</td>\n",
       "      <td>111977.548036</td>\n",
       "      <td>4658.524473</td>\n",
       "      <td>...</td>\n",
       "      <td>1.212809</td>\n",
       "      <td>52.325077</td>\n",
       "      <td>-4.065605</td>\n",
       "      <td>54.717674</td>\n",
       "      <td>0.669643</td>\n",
       "      <td>54.804890</td>\n",
       "      <td>-2.393862</td>\n",
       "      <td>57.423059</td>\n",
       "      <td>-1.166289</td>\n",
       "      <td>59.186117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>661794.000000</td>\n",
       "      <td>0.435942</td>\n",
       "      <td>0.091256</td>\n",
       "      <td>0.175682</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>2691.294667</td>\n",
       "      <td>6.121479e+05</td>\n",
       "      <td>2578.469836</td>\n",
       "      <td>182371.576801</td>\n",
       "      <td>5533.810460</td>\n",
       "      <td>...</td>\n",
       "      <td>4.359662</td>\n",
       "      <td>71.691755</td>\n",
       "      <td>-0.838737</td>\n",
       "      <td>75.040838</td>\n",
       "      <td>3.119212</td>\n",
       "      <td>75.385832</td>\n",
       "      <td>0.150573</td>\n",
       "      <td>78.626444</td>\n",
       "      <td>1.312615</td>\n",
       "      <td>85.375374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>675808.000000</td>\n",
       "      <td>0.663685</td>\n",
       "      <td>0.108111</td>\n",
       "      <td>0.397973</td>\n",
       "      <td>0.027679</td>\n",
       "      <td>4435.243901</td>\n",
       "      <td>3.036843e+06</td>\n",
       "      <td>3509.646417</td>\n",
       "      <td>694784.811549</td>\n",
       "      <td>8677.672688</td>\n",
       "      <td>...</td>\n",
       "      <td>13.457150</td>\n",
       "      <td>392.932373</td>\n",
       "      <td>11.482946</td>\n",
       "      <td>406.058868</td>\n",
       "      <td>15.388390</td>\n",
       "      <td>332.905426</td>\n",
       "      <td>14.694924</td>\n",
       "      <td>393.161987</td>\n",
       "      <td>15.369627</td>\n",
       "      <td>506.065155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              length  chroma_stft_mean  chroma_stft_var     rms_mean  \\\n",
       "count    1000.000000       1000.000000      1000.000000  1000.000000   \n",
       "mean   662030.846000          0.378682         0.086340     0.130930   \n",
       "std      1784.073992          0.081705         0.007735     0.065683   \n",
       "min    660000.000000          0.171939         0.044555     0.005276   \n",
       "25%    661504.000000          0.319562         0.082298     0.086657   \n",
       "50%    661794.000000          0.383148         0.086615     0.122443   \n",
       "75%    661794.000000          0.435942         0.091256     0.175682   \n",
       "max    675808.000000          0.663685         0.108111     0.397973   \n",
       "\n",
       "           rms_var  spectral_centroid_mean  spectral_centroid_var  \\\n",
       "count  1000.000000             1000.000000           1.000000e+03   \n",
       "mean      0.003051             2201.780898           4.696916e+05   \n",
       "std       0.003634              715.960600           4.008995e+05   \n",
       "min       0.000004              570.040355           7.911251e+03   \n",
       "25%       0.000942             1627.697311           1.843505e+05   \n",
       "50%       0.001816             2209.263090           3.384862e+05   \n",
       "75%       0.003577             2691.294667           6.121479e+05   \n",
       "max       0.027679             4435.243901           3.036843e+06   \n",
       "\n",
       "       spectral_bandwidth_mean  spectral_bandwidth_var  rolloff_mean  ...  \\\n",
       "count              1000.000000             1000.000000   1000.000000  ...   \n",
       "mean               2242.541070           137079.155165   4571.549304  ...   \n",
       "std                 526.316473            96455.666326   1574.791602  ...   \n",
       "min                 898.066208            10787.185064    749.140636  ...   \n",
       "25%                1907.240605            67376.554428   3380.069642  ...   \n",
       "50%                2221.392843           111977.548036   4658.524473  ...   \n",
       "75%                2578.469836           182371.576801   5533.810460  ...   \n",
       "max                3509.646417           694784.811549   8677.672688  ...   \n",
       "\n",
       "       mfcc16_mean   mfcc16_var  mfcc17_mean   mfcc17_var  mfcc18_mean  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      1.148144    60.730958    -3.966028    62.633624     0.507696   \n",
       "std       4.578948    33.781951     4.549697    33.479172     3.869105   \n",
       "min     -15.693844     9.169314   -17.234728    13.931521   -11.963694   \n",
       "25%      -1.863280    40.376442    -7.207225    40.830875    -2.007015   \n",
       "50%       1.212809    52.325077    -4.065605    54.717674     0.669643   \n",
       "75%       4.359662    71.691755    -0.838737    75.040838     3.119212   \n",
       "max      13.457150   392.932373    11.482946   406.058868    15.388390   \n",
       "\n",
       "        mfcc18_var  mfcc19_mean   mfcc19_var  mfcc20_mean   mfcc20_var  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean     63.712586    -2.328761    66.231930    -1.095348    70.126096  \n",
       "std      34.401977     3.755957    37.174631     3.837007    45.228512  \n",
       "min      15.420555   -18.501955    13.487622   -19.929634     7.956583  \n",
       "25%      41.884240    -4.662925    41.710184    -3.368996    42.372865  \n",
       "50%      54.804890    -2.393862    57.423059    -1.166289    59.186117  \n",
       "75%      75.385832     0.150573    78.626444     1.312615    85.375374  \n",
       "max     332.905426    14.694924   393.161987    15.369627   506.065155  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cddd54a1-f5e9-4b07-bd53-29386db18eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 59)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06a57a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000 entries, blues.00000.wav to rock.00099.wav\n",
      "Data columns (total 59 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   length                   1000 non-null   int64  \n",
      " 1   chroma_stft_mean         1000 non-null   float64\n",
      " 2   chroma_stft_var          1000 non-null   float64\n",
      " 3   rms_mean                 1000 non-null   float64\n",
      " 4   rms_var                  1000 non-null   float64\n",
      " 5   spectral_centroid_mean   1000 non-null   float64\n",
      " 6   spectral_centroid_var    1000 non-null   float64\n",
      " 7   spectral_bandwidth_mean  1000 non-null   float64\n",
      " 8   spectral_bandwidth_var   1000 non-null   float64\n",
      " 9   rolloff_mean             1000 non-null   float64\n",
      " 10  rolloff_var              1000 non-null   float64\n",
      " 11  zero_crossing_rate_mean  1000 non-null   float64\n",
      " 12  zero_crossing_rate_var   1000 non-null   float64\n",
      " 13  harmony_mean             1000 non-null   float64\n",
      " 14  harmony_var              1000 non-null   float64\n",
      " 15  perceptr_mean            1000 non-null   float64\n",
      " 16  perceptr_var             1000 non-null   float64\n",
      " 17  tempo                    1000 non-null   float64\n",
      " 18  mfcc1_mean               1000 non-null   float64\n",
      " 19  mfcc1_var                1000 non-null   float64\n",
      " 20  mfcc2_mean               1000 non-null   float64\n",
      " 21  mfcc2_var                1000 non-null   float64\n",
      " 22  mfcc3_mean               1000 non-null   float64\n",
      " 23  mfcc3_var                1000 non-null   float64\n",
      " 24  mfcc4_mean               1000 non-null   float64\n",
      " 25  mfcc4_var                1000 non-null   float64\n",
      " 26  mfcc5_mean               1000 non-null   float64\n",
      " 27  mfcc5_var                1000 non-null   float64\n",
      " 28  mfcc6_mean               1000 non-null   float64\n",
      " 29  mfcc6_var                1000 non-null   float64\n",
      " 30  mfcc7_mean               1000 non-null   float64\n",
      " 31  mfcc7_var                1000 non-null   float64\n",
      " 32  mfcc8_mean               1000 non-null   float64\n",
      " 33  mfcc8_var                1000 non-null   float64\n",
      " 34  mfcc9_mean               1000 non-null   float64\n",
      " 35  mfcc9_var                1000 non-null   float64\n",
      " 36  mfcc10_mean              1000 non-null   float64\n",
      " 37  mfcc10_var               1000 non-null   float64\n",
      " 38  mfcc11_mean              1000 non-null   float64\n",
      " 39  mfcc11_var               1000 non-null   float64\n",
      " 40  mfcc12_mean              1000 non-null   float64\n",
      " 41  mfcc12_var               1000 non-null   float64\n",
      " 42  mfcc13_mean              1000 non-null   float64\n",
      " 43  mfcc13_var               1000 non-null   float64\n",
      " 44  mfcc14_mean              1000 non-null   float64\n",
      " 45  mfcc14_var               1000 non-null   float64\n",
      " 46  mfcc15_mean              1000 non-null   float64\n",
      " 47  mfcc15_var               1000 non-null   float64\n",
      " 48  mfcc16_mean              1000 non-null   float64\n",
      " 49  mfcc16_var               1000 non-null   float64\n",
      " 50  mfcc17_mean              1000 non-null   float64\n",
      " 51  mfcc17_var               1000 non-null   float64\n",
      " 52  mfcc18_mean              1000 non-null   float64\n",
      " 53  mfcc18_var               1000 non-null   float64\n",
      " 54  mfcc19_mean              1000 non-null   float64\n",
      " 55  mfcc19_var               1000 non-null   float64\n",
      " 56  mfcc20_mean              1000 non-null   float64\n",
      " 57  mfcc20_var               1000 non-null   float64\n",
      " 58  label                    1000 non-null   object \n",
      "dtypes: float64(57), int64(1), object(1)\n",
      "memory usage: 468.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bcd2a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length', 'chroma_stft_mean', 'chroma_stft_var', 'rms_mean', 'rms_var', 'spectral_centroid_mean', 'spectral_centroid_var', 'spectral_bandwidth_mean', 'spectral_bandwidth_var', 'rolloff_mean', 'rolloff_var', 'zero_crossing_rate_mean', 'zero_crossing_rate_var', 'harmony_mean', 'harmony_var', 'perceptr_mean', 'perceptr_var', 'tempo', 'mfcc1_mean', 'mfcc1_var', 'mfcc2_mean', 'mfcc2_var', 'mfcc3_mean', 'mfcc3_var', 'mfcc4_mean', 'mfcc4_var', 'mfcc5_mean', 'mfcc5_var', 'mfcc6_mean', 'mfcc6_var', 'mfcc7_mean', 'mfcc7_var', 'mfcc8_mean', 'mfcc8_var', 'mfcc9_mean', 'mfcc9_var', 'mfcc10_mean', 'mfcc10_var', 'mfcc11_mean', 'mfcc11_var', 'mfcc12_mean', 'mfcc12_var', 'mfcc13_mean', 'mfcc13_var', 'mfcc14_mean', 'mfcc14_var', 'mfcc15_mean', 'mfcc15_var', 'mfcc16_mean', 'mfcc16_var', 'mfcc17_mean', 'mfcc17_var', 'mfcc18_mean', 'mfcc18_var', 'mfcc19_mean', 'mfcc19_var', 'mfcc20_mean', 'mfcc20_var', 'label']\n"
     ]
    }
   ],
   "source": [
    "cols = df.columns.values.tolist()\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd3317-d2e6-4f2e-aeff-4e0ec4b9b0ad",
   "metadata": {},
   "source": [
    "#### Check data for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "652b3594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical: ['label']\n",
      "Numerical: ['length', 'chroma_stft_mean', 'chroma_stft_var', 'rms_mean', 'rms_var', 'spectral_centroid_mean', 'spectral_centroid_var', 'spectral_bandwidth_mean', 'spectral_bandwidth_var', 'rolloff_mean', 'rolloff_var', 'zero_crossing_rate_mean', 'zero_crossing_rate_var', 'harmony_mean', 'harmony_var', 'perceptr_mean', 'perceptr_var', 'tempo', 'mfcc1_mean', 'mfcc1_var', 'mfcc2_mean', 'mfcc2_var', 'mfcc3_mean', 'mfcc3_var', 'mfcc4_mean', 'mfcc4_var', 'mfcc5_mean', 'mfcc5_var', 'mfcc6_mean', 'mfcc6_var', 'mfcc7_mean', 'mfcc7_var', 'mfcc8_mean', 'mfcc8_var', 'mfcc9_mean', 'mfcc9_var', 'mfcc10_mean', 'mfcc10_var', 'mfcc11_mean', 'mfcc11_var', 'mfcc12_mean', 'mfcc12_var', 'mfcc13_mean', 'mfcc13_var', 'mfcc14_mean', 'mfcc14_var', 'mfcc15_mean', 'mfcc15_var', 'mfcc16_mean', 'mfcc16_var', 'mfcc17_mean', 'mfcc17_var', 'mfcc18_mean', 'mfcc18_var', 'mfcc19_mean', 'mfcc19_var', 'mfcc20_mean', 'mfcc20_var']\n"
     ]
    }
   ],
   "source": [
    "print(\"Categorical:\", list(df.select_dtypes(include=['object']).columns))\n",
    "print(\"Numerical:\", list(df.select_dtypes(exclude=['object']).columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49ae92ee-62de-42c4-8b60-57b5272a5254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there are any missing values? No\n",
      "Features with missing values []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique</th>\n",
       "      <th>total_null</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chroma_stft_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chroma_stft_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rms_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rms_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectral_centroid_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectral_bandwidth_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectral_bandwidth_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rolloff_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rolloff_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero_crossing_rate_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero_crossing_rate_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harmony_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harmony_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perceptr_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perceptr_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempo</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc1_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc1_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc2_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc2_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc3_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc3_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc4_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc4_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc5_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc5_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc6_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc6_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc7_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc7_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc8_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc8_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc9_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc9_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc10_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc10_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc11_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc11_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc12_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc12_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc13_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc13_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc14_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc14_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc15_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc15_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc16_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc16_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc17_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc17_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc18_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc18_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc19_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc19_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc20_mean</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfcc20_var</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         unique  total_null  percentage\n",
       "length                        0         0.0          35\n",
       "chroma_stft_mean              0         0.0         986\n",
       "chroma_stft_var               0         0.0         986\n",
       "rms_mean                      0         0.0         986\n",
       "rms_var                       0         0.0         986\n",
       "spectral_centroid_mean        0         0.0         986\n",
       "spectral_centroid_var         0         0.0         986\n",
       "spectral_bandwidth_mean       0         0.0         986\n",
       "spectral_bandwidth_var        0         0.0         986\n",
       "rolloff_mean                  0         0.0         986\n",
       "rolloff_var                   0         0.0         986\n",
       "zero_crossing_rate_mean       0         0.0         984\n",
       "zero_crossing_rate_var        0         0.0         986\n",
       "harmony_mean                  0         0.0         986\n",
       "harmony_var                   0         0.0         986\n",
       "perceptr_mean                 0         0.0         986\n",
       "perceptr_var                  0         0.0         986\n",
       "tempo                         0         0.0          35\n",
       "mfcc1_mean                    0         0.0         986\n",
       "mfcc1_var                     0         0.0         986\n",
       "mfcc2_mean                    0         0.0         986\n",
       "mfcc2_var                     0         0.0         986\n",
       "mfcc3_mean                    0         0.0         986\n",
       "mfcc3_var                     0         0.0         986\n",
       "mfcc4_mean                    0         0.0         986\n",
       "mfcc4_var                     0         0.0         986\n",
       "mfcc5_mean                    0         0.0         986\n",
       "mfcc5_var                     0         0.0         986\n",
       "mfcc6_mean                    0         0.0         986\n",
       "mfcc6_var                     0         0.0         986\n",
       "mfcc7_mean                    0         0.0         986\n",
       "mfcc7_var                     0         0.0         986\n",
       "mfcc8_mean                    0         0.0         986\n",
       "mfcc8_var                     0         0.0         986\n",
       "mfcc9_mean                    0         0.0         986\n",
       "mfcc9_var                     0         0.0         986\n",
       "mfcc10_mean                   0         0.0         986\n",
       "mfcc10_var                    0         0.0         986\n",
       "mfcc11_mean                   0         0.0         986\n",
       "mfcc11_var                    0         0.0         986\n",
       "mfcc12_mean                   0         0.0         986\n",
       "mfcc12_var                    0         0.0         986\n",
       "mfcc13_mean                   0         0.0         986\n",
       "mfcc13_var                    0         0.0         986\n",
       "mfcc14_mean                   0         0.0         986\n",
       "mfcc14_var                    0         0.0         986\n",
       "mfcc15_mean                   0         0.0         986\n",
       "mfcc15_var                    0         0.0         986\n",
       "mfcc16_mean                   0         0.0         986\n",
       "mfcc16_var                    0         0.0         986\n",
       "mfcc17_mean                   0         0.0         986\n",
       "mfcc17_var                    0         0.0         986\n",
       "mfcc18_mean                   0         0.0         986\n",
       "mfcc18_var                    0         0.0         986\n",
       "mfcc19_mean                   0         0.0         986\n",
       "mfcc19_var                    0         0.0         986\n",
       "mfcc20_mean                   0         0.0         986\n",
       "mfcc20_var                    0         0.0         986\n",
       "label                         0         0.0          10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Are there are any missing values?\", 'Yes' if df.isnull().values.any() else 'No')\n",
    "unique = df.nunique(axis=0)\n",
    "total_null = df.isnull().sum()\n",
    "print(\"Features with missing values\", df.columns[df.isnull().any()].tolist())\n",
    "percentage = (df.isnull().sum().sort_values(ascending=False)/len(df))*100\n",
    "out = pd.concat([total_null,percentage, unique],axis=1, keys=[\"unique\",\"total_null\",\"percentage\"])\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2d45a-ed4c-4dad-9346-d8a83fd51fbb",
   "metadata": {},
   "source": [
    "### Do we need scaling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849685ba-0ae0-47de-9934-0af5938b9495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1b892df-8faa-4e4f-b673-cb3a57b5014b",
   "metadata": {},
   "source": [
    "### Explore the relationship between the target label and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9785d5-56fb-4d17-a1a3-f74ff4293f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDEs\n",
    "# Boxplots\n",
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f69d0a-36e4-4b92-beb2-70f117ffa38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_30.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cbc97a-e7aa-4693-89de-dbdd92d1e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr_matrix, cmap = 'vlag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc383e-7ea6-4cd9-aa41-3ba3f6a542d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### cite : https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on\n",
    "\n",
    "###taken from the stackoverflow code on how to drop highly correlated feature columns\n",
    "high = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "correlated_cols = [column for column in high.columns if any(high[column] > 0.90)]\n",
    "correlated_cols\n",
    "#df_30.drop(correlated_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a29944b-54bc-49f7-ab73-17856adb13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "\n",
    "fig,ax = plt.subplots(12,5, figsize=(40,34), sharey=True)\n",
    "ax = ax.ravel()\n",
    "feature_list = list(df_30.columns)\n",
    "feature_list.remove('label')\n",
    "# print(feature_list)\n",
    "for i in range(0, len(feature_list)):\n",
    "    sns.boxplot(x = df_30[str(feature_list[i])], y = df_30['label'], data = df_30, ax = ax[i])\n",
    "    ax[i].set_title(\"Label vs \" + feature_list[i])\n",
    "plt.show()\n",
    "fig.suptitle('Relationship between the Label and %f features'%(len(feature_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50afaea9-de60-4a46-ac00-fb2a6130766d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "\n",
    "feature_list = list(df_30.columns)\n",
    "feature_list.remove('label')\n",
    "for i in range(0, len(feature_list)):\n",
    "    sns.boxplot(x = df_30[str(feature_list[i])], y = df_30['label'], data = df_30)\n",
    "    plt.title(\"Label vs \" + feature_list[i])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e30cf5-117b-490d-9247-3bed61bee4b2",
   "metadata": {},
   "source": [
    "### soundwaves and spectograms for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f1d83-c1d7-4935-959a-625a70bba5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee536a-c286-418d-b6ba-e0fcf7c17e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e798b-abf7-4002-810d-66456ee95812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43530c9-5abe-442b-aa9c-5c97f9bd2ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2be3e924",
   "metadata": {
    "id": "2be3e924"
   },
   "source": [
    "**1.1. Plot z vs x and z vs y in the synthetic dataset as scatter plots. Label your axes and make sure your y-axis starts from 0. Do the independent and dependent features have linear relationship?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5145cad",
   "metadata": {
    "id": "c5145cad"
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "plt.scatter(x,z, c =\"blue\", \n",
    "            linewidths = 2, \n",
    "            marker =\"o\", \n",
    "            edgecolor =\"green\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"z\")\n",
    "plt.xlim(0)\n",
    "plt.ylim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94433e8e",
   "metadata": {
    "id": "94433e8e"
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "plt.scatter(y,z, c =\"green\", \n",
    "            linewidths = 2, \n",
    "            marker =\"x\", \n",
    "            edgecolor =\"yellow\")\n",
    "\n",
    "plt.xlabel(\"y\")\n",
    "plt.ylabel(\"z\")\n",
    "plt.xlim(0)\n",
    "plt.ylim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86117afe",
   "metadata": {},
   "source": [
    "**Ans 1.1** The independent and the dependent variables are linearly related as we can see from the above two scatterplots. The relation is positive linear relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbb473b",
   "metadata": {
    "id": "0dbb473b"
   },
   "source": [
    "**1.2. Are the independent variables correlated? Use pearson correlation to verify? What would be the problem if linear regression is applied to correlated features?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ba11c",
   "metadata": {
    "id": "445ba11c"
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "plt.scatter(x,y, c =\"black\", \n",
    "            linewidths = 1, \n",
    "            marker =\"o\", \n",
    "            edgecolor =\"blue\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim(0)\n",
    "plt.ylim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e952316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###verify with the help of pearson \n",
    "\n",
    "import scipy\n",
    "covar = np.cov(x,y) ###cov\n",
    "print(\"Covariance \\n\", covar)\n",
    "\n",
    "corrn = np.corrcoef(x,y) ###finding correlation between two independent variables \n",
    "print(\"Correlation between the independent variables :\\n\", corrn)\n",
    "\n",
    "pearson_cov = scipy.stats.pearsonr(x,y) ###pearson between independent features\n",
    "print(\"Pearson correlation :\\n\", pearson_cov)\n",
    "\n",
    "#plot the correlation and covariance\n",
    "fig,(ax1,ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "fig.suptitle('Correlation Plot and Covariance Plot')\n",
    "ax1 = sns.heatmap(corrn, annot=True, ax=ax1, annot_kws=dict(clip_on=True), cmap=plt.cm.Reds)\n",
    "ax2 = sns.heatmap(covar, annot=True, ax=ax2, annot_kws=dict(clip_on=True), cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999d026",
   "metadata": {},
   "source": [
    "**Ans :** The variables are indeed correlated as seen from the above values of correlation matrix. The correlation matrix and the pearson coefficient both shows the strong correlation, as we know that if r (coefficient) > 0.5 it signifies strong association between the features. Ideally, the independent variables should be independent but this strong correlation will effect the features such that even a slight change will effect the other correlated features, hence the problem of ***Multicollinearity*** might generate, which will eventually cause fluctuations in the model training hence leading to unstable model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57cd121",
   "metadata": {
    "id": "d57cd121"
   },
   "source": [
    "**The second dataset we will be using is an auto MPG dataset. This dataset contains various characteristics for around 8128 cars. We will use linear regression to predict the selling_price label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6lnQQunWxCx",
   "metadata": {
    "id": "d6lnQQunWxCx"
   },
   "outputs": [],
   "source": [
    "auto_mpg_df = pd.read_csv('Car details v3.csv')\n",
    "# Dropping Torque column, there is information in this column but it will take some preprocessing.\n",
    "# The idea of the exercise is to familarize yourself with the basics of Linear regression.\n",
    "auto_mpg_df = auto_mpg_df.drop(['torque'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01532cf",
   "metadata": {
    "id": "d01532cf",
    "outputId": "ea9cb509-bcc8-46bb-db9d-b1e8f71def48"
   },
   "outputs": [],
   "source": [
    "auto_mpg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcccde60",
   "metadata": {
    "id": "bcccde60"
   },
   "source": [
    "**1.3. Missing Value analysis - Auto mpg dataset.**\n",
    "\n",
    "**Are there any missing values in the dataset? If so, what can be done about it? Jusify your approach.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136afc9",
   "metadata": {
    "id": "7136afc9"
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "print(\"Are there are any null values :\", auto_mpg_df.isnull().values.any())\n",
    "unique = auto_mpg_df.nunique(axis=0)\n",
    "total_null = auto_mpg_df.isnull().sum()\n",
    "print(\"NaN values features\", auto_mpg_df.columns[auto_mpg_df.isnull().any()].tolist() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62be6c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###some basic information of the dataset, also to figure out how many of them are categorical or numerical\n",
    "\n",
    "df = auto_mpg_df  ###for my own ease\n",
    "df.info()\n",
    "print(\"Categorical:\", list(df.select_dtypes(include=['object']).columns))\n",
    "print(\"Numerical:\", list(df.select_dtypes(exclude=['object']).columns))\n",
    "print(\"\\n\")\n",
    "t1 = pd.concat([unique, total_null],axis =1, keys  =['unique','total_null'])\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6fe04",
   "metadata": {},
   "source": [
    "**Ans :** Yes, there are a couple of columns(features) that have missing values in them. \n",
    "They are ['mileage', 'engine', 'max_power', 'seats']. \n",
    "There are a couple of approaches that are used to deal with the missing dataset which depends on the type of the feature. For instance for the categorical features we can either drop the rows, or the missing data, impute it etc, whereas for the numerical features, replacing the missing data with the mean/mode/median depending on the type of the values of the features. Dropping the features definitly causes the effect on the accuracy of the models unless the feature is of not much importance like \"name\", hence it is better to approach categorical features by encoding them, and converting them to the numerical features and then imputing/standardizing them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809bb8e",
   "metadata": {
    "id": "d809bb8e"
   },
   "source": [
    "**1.4. The features engine, max_power and mileage have units in the dataset. In the real world if we have such datasets, we generally remove the units from each feature. After doing so, convert the datatype of these columns to float. For example: 1248 CC engine is 1248, 23.4 kmpl is 23.4 and so on.**\n",
    "\n",
    "**Hint: Check for distinct units in each of these features. A feature might have multiple units as well. Also, a feature could have no value but have unit. For example 'CC' without any value. Remove such rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b76d44",
   "metadata": {
    "id": "21b76d44"
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "##split the string in two and check for the units by using unique\n",
    "\n",
    "df[['engine','eunit']] = auto_mpg_df[\"engine\"].str.split(\" \", expand=True)\n",
    "df[['mileage','munit']] = auto_mpg_df[\"mileage\"].str.split(\" \", expand=True)\n",
    "df[['max_power','punit']] = auto_mpg_df[\"max_power\"].str.split(\" \", expand=True)\n",
    "\n",
    "print(\"Engine Units : \",df['eunit'].unique())\n",
    "print(\"Mileage Units : \",df['munit'].unique())\n",
    "print(\"Max Power Units : \",df['punit'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972e805",
   "metadata": {},
   "source": [
    "***Now we know which of these have multiple units, so we can replace the units and keep the numerical values and convert to float***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d591ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the units with the \"\" \n",
    "\n",
    "auto_mpg_df['engine'] = auto_mpg_df[\"engine\"].replace(' CC','',regex=True)\n",
    "auto_mpg_df['mileage'] = auto_mpg_df[\"mileage\"].replace(' kmpl','',regex=True)\n",
    "auto_mpg_df['mileage'] = auto_mpg_df[\"mileage\"].replace(' km/kg','',regex=True)\n",
    "auto_mpg_df['max_power'] = auto_mpg_df[\"max_power\"].replace(' bhp','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaadc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "\n",
    "print(\"Null values in Engine\", auto_mpg_df['engine'].isnull().sum())\n",
    "print(\"Null values in Mileage\", auto_mpg_df['mileage'].isnull().sum())\n",
    "print(\"Null values in Max_Power\", auto_mpg_df['max_power'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f78e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_df['engine'] = pd.to_numeric(auto_mpg_df['engine'], errors=\"raise\")\n",
    "auto_mpg_df['mileage'] = pd.to_numeric(auto_mpg_df['mileage'], errors=\"raise\")\n",
    "auto_mpg_df['max_power'] = pd.to_numeric(auto_mpg_df['max_power'], errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##drop extra unit columns\n",
    "\n",
    "auto_mpg_df = auto_mpg_df.drop(columns=['eunit','punit','munit'])\n",
    "auto_mpg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef346ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###verify datatype of the mileage, max_power and mileage\n",
    "\n",
    "print(\"Data Type of mileage:\", auto_mpg_df['mileage'].dtypes)\n",
    "print(\"Data Type of mileage:\", auto_mpg_df['engine'].dtypes)\n",
    "print(\"Data Type of mileage:\", auto_mpg_df['max_power'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41705767",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg_df = auto_mpg_df.dropna(how='any',axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e911550",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auto_mpg_df.isnull().any())\n",
    "auto_mpg_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d73f9",
   "metadata": {
    "id": "862d73f9"
   },
   "outputs": [],
   "source": [
    "auto_mpg_X = auto_mpg_df.drop(columns=['selling_price'])\n",
    "auto_mpg_y = auto_mpg_df['selling_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881aff2",
   "metadata": {
    "id": "e881aff2"
   },
   "source": [
    "**1.5. Plot the distribution of the label (selling_price) using a histogram. Make multiple plots with different binwidths. Make sure to label your axes while plotting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f96e30",
   "metadata": {
    "id": "a0f96e30"
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})  ##cite : stackoverflow \"how to change figure size of sns article\"\n",
    "\n",
    "fig,ax = plt.subplots(2, 2, figsize=(15,10))\n",
    "fig.suptitle('Label \"Selling Price\" with different bin width')\n",
    "ax = ax.ravel()\n",
    "binsize =[10000,50000,100000,1000000]\n",
    "for i in range(0,len(binsize)):\n",
    "    sns.histplot(ax=ax[i],data=auto_mpg_y,x=auto_mpg_y,binwidth=binsize[i])\n",
    "    ax[i].set_title(\"Binwidth : \" + str(binsize[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b87f7d8",
   "metadata": {
    "id": "5b87f7d8"
   },
   "source": [
    "**1.6. Plot the relationships between the label (Selling Price) and the continuous features (Mileage, km driven, engine, max power) using a small multiple of scatter plots. \n",
    "Make sure to label the axes. Do you see something interesting about the distributions of these features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b62169",
   "metadata": {
    "id": "c3b62169"
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "fig,ax = plt.subplots(2, 2, figsize=(15,10), sharey=True)\n",
    "ax = ax.ravel()\n",
    "fig.suptitle('Label \"Selling Price\" with continuous features')\n",
    "cfeatures = ['mileage','km_driven','engine','max_power']\n",
    "for i in range(0,len(cfeatures)):\n",
    "    sns.scatterplot(x=auto_mpg_X[str(cfeatures[i])],y=auto_mpg_y, data=auto_mpg_X, ax=ax[i])\n",
    "    ax[i].set_title(\"selling_price vs \" + cfeatures[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ffa354",
   "metadata": {
    "id": "84ffa354"
   },
   "source": [
    "**1.7. Plot the relationships between the label (Selling Price) and the discrete features (fuel type, Seller type, transmission) using a small multiple of box plots. Make sure to label the axes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16812f56",
   "metadata": {
    "id": "16812f56"
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "fig,ax = plt.subplots(1,3, figsize=(15,5), sharey=True)\n",
    "ax = ax.ravel()\n",
    "fig.suptitle('Label \"Selling Price\" with Discrete features')\n",
    "dfeatures = ['fuel','seller_type','transmission']\n",
    "for i in range(0,len(dfeatures)):\n",
    "    sns.boxplot(x=auto_mpg_X[str(dfeatures[i])],y =auto_mpg_y, data=auto_mpg_X, ax=ax[i])\n",
    "    ax[i].set_title(\"selling_price vs \" + dfeatures[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eaf41d",
   "metadata": {
    "id": "28eaf41d"
   },
   "source": [
    "**1.8. From the visualizations above, do you think linear regression is a good model for this problem? Why and/or why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba830e0",
   "metadata": {
    "id": "50c93674"
   },
   "source": [
    "### Your answer here\n",
    " \n",
    "Yes, the linear regression can be a good choice of model for this dataset, since most of the features are falling in the same line, there are a couple of outliers but they can be removed or data can be standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7fb6c1",
   "metadata": {
    "id": "1c7fb6c1"
   },
   "outputs": [],
   "source": [
    "auto_mpg_X['year'] =  2020 - auto_mpg_X['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdee656",
   "metadata": {
    "id": "4fdee656",
    "outputId": "c91123e2-48c1-4046-d283-3e54926a9a8a"
   },
   "outputs": [],
   "source": [
    "#dropping the car name as it is irrelevant.\n",
    "auto_mpg_X.drop(['name'],axis = 1,inplace=True)\n",
    "\n",
    "#check out the dataset with new changes\n",
    "auto_mpg_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ebca87",
   "metadata": {
    "id": "71ebca87"
   },
   "source": [
    "**Data Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980d1e4",
   "metadata": {
    "id": "6980d1e4"
   },
   "source": [
    "**1.9.\n",
    "Before we can fit a linear regression model, there are several pre-processing steps we should apply to the datasets:**\n",
    "1. Encode categorial features appropriately.\n",
    "2. Split the dataset into training (60%), validation (20%), and test (20%) sets.\n",
    "3. Standardize the columns in the feature matrices X_train, X_val, and X_test to have zero mean and unit variance. To avoid information leakage, learn the standardization parameters (mean, variance) from X_train, and apply it to X_train, X_val, and X_test.\n",
    "4. Add a column of ones to the feature matrices X_train, X_val, and X_test. This is a common trick so that we can learn a coefficient for the bias term of a linear model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8711d6a7",
   "metadata": {
    "id": "8711d6a7"
   },
   "outputs": [],
   "source": [
    "# 1. No categorical features in the synthetic dataset (skip this step)\n",
    "\n",
    "# 2. Split the dataset into training (60%), validation (20%), and test (20%) sets\n",
    "\n",
    "# 3. Standardize the columns in the feature matrices\n",
    "\n",
    "# 4. Add a column of ones to the feature matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac56dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we see 4 categorical features hence we can one hot encode them step 1\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "auto_mpg_X_encoded = auto_mpg_X.copy()\n",
    "auto_mpg_X_encoded=pd.get_dummies(auto_mpg_X_encoded, columns=['seller_type'])\n",
    "auto_mpg_X_encoded=pd.get_dummies(auto_mpg_X_encoded, columns=['fuel'])\n",
    "auto_mpg_X_encoded=pd.get_dummies(auto_mpg_X_encoded, columns=['transmission'])\n",
    "auto_mpg_X_encoded=pd.get_dummies(auto_mpg_X_encoded, columns=['owner'])\n",
    "auto_mpg_X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### step 2, split them\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "auto_mpg_y = np.log(auto_mpg_y.to_numpy())\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(auto_mpg_X_encoded, auto_mpg_y, test_size=0.2, shuffle = True) #test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.25, shuffle = True) #val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### step 3 standardize in the feature matrices\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "###standardize the dataset \n",
    "###cite : https://stackoverflow.com/questions/24645153/pandas-dataframe-columns-scaling-with-sklearn and https://www.kaggle.com/viswanathanc/auto-mpg-linear-regression\n",
    "ta1 = X_train.columns\n",
    "ta2 = X_test.columns\n",
    "ta3 = X_val.columns\n",
    "X_train[ta1] = StandardScaler().fit_transform(X_train[ta1])\n",
    "X_test[ta2] = StandardScaler().fit_transform(X_test[ta2])\n",
    "X_val[ta3] = StandardScaler().fit_transform(X_val[ta3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae89cee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"testing standardizing on X_train:\", X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "### step 4 add bias as one more col this will make the size as mxn where n = 21\n",
    "X_train['bias']=1\n",
    "X_test['bias']=1\n",
    "X_val['bias']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da674d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train shape:\",X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "print(\"Val shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f6e97",
   "metadata": {
    "id": "950f6e97"
   },
   "source": [
    "**At the end of this pre-processing, you should have the following vectors and matrices:**\n",
    "\n",
    "**- Auto MPG dataset: auto_mpg_X_train, auto_mpg_X_val, auto_mpg_X_test, auto_mpg_y_train, auto_mpg_y_val, auto_mpg_y_test**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b7ca8",
   "metadata": {
    "id": "1e3b7ca8"
   },
   "source": [
    "**Implement Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b659a6",
   "metadata": {
    "id": "63b659a6"
   },
   "source": [
    "Now, we can implement our linear regression model! Specifically, we will be implementing ridge regression, which is linear regression with L2 regularization. Given an (m x n) feature matrix $X$, an (m x 1) label vector $y$, and an (n x 1) weight vector $w$, the hypothesis function for linear regression is:\n",
    "\n",
    "$$\n",
    "y = X w\n",
    "$$\n",
    "\n",
    "Note that we can omit the bias term here because we have included a column of ones in our $X$ matrix, so the bias term is learned implicitly as a part of $w$. This will make our implementation easier.\n",
    "\n",
    "Our objective in linear regression is to learn the weights $w$ which best fit the data. This notion can be formalized as finding the optimal $w$ which minimizes the following loss function:\n",
    "\n",
    "$$\n",
    "\\min_{w} \\| X w - y \\|^2_2 + \\alpha \\| w \\|^2_2 \\\\\n",
    "$$\n",
    "\n",
    "This is the ridge regression loss function. The $\\| X w - y \\|^2_2$ term penalizes predictions $Xw$ which are not close to the label $y$. And the $\\alpha \\| w \\|^2_2$ penalizes large weight values, to favor a simpler, more generalizable model. The $\\alpha$ hyperparameter, known as the regularization parameter, is used to tune the complexity of the model - a higher $\\alpha$ results in smaller weights and lower complexity, and vice versa. Setting $\\alpha = 0$ gives us vanilla linear regression.\n",
    "\n",
    "Conveniently, ridge regression has a closed-form solution which gives us the optimal $w$ without having to do iterative methods such as gradient descent. The closed-form solution, known as the Normal Equations, is given by:\n",
    "\n",
    "$$\n",
    "w = (X^T X + \\alpha I)^{-1} X^T y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e132b4",
   "metadata": {
    "id": "a7e132b4"
   },
   "source": [
    "**1.10. Implement a `LinearRegression` class with two methods: `train` and `predict`. You may NOT use sklearn for this implementation. You may, however, use `np.linalg.solve` to find the closed-form solution. It is highly recommended that you vectorize your code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8cee4a",
   "metadata": {
    "id": "2e8cee4a"
   },
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    '''\n",
    "    Linear regression model with L2-regularization (i.e. ridge regression).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    alpha: regularization parameter\n",
    "    w: (n x 1) weight vector\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, alpha=0):\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        '''Trains model using ridge regression closed-form solution \n",
    "        (sets w to its optimal value).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        y: (m x 1) label vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        n = X.shape[1]\n",
    "        I = np.identity(n)\n",
    "        xtx = np.dot(np.transpose(X),X)\n",
    "        xy = np.dot(np.transpose(X),y)\n",
    "        I_alpha = self.alpha * I\n",
    "        xtx_dash = xtx + I_alpha\n",
    "        self.w = np.dot(np.linalg.pinv(xtx_dash),xy)\n",
    "        return self.w\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''Predicts on X using trained model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: (m x 1) prediction vector\n",
    "        '''\n",
    "        ### Your code here\n",
    "        y_pred=np.dot(X,self.w)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a741e",
   "metadata": {
    "id": "c48a741e"
   },
   "source": [
    "**Train, Evaluate, and Interpret Linear Regression Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ae0161",
   "metadata": {
    "id": "18ae0161"
   },
   "source": [
    "**1.11. A) Train a linear regression model ($\\alpha = 0$) on the auto MPG training data. Make predictions and report the mean-squared error (MSE) on the training, validation, and test sets. Report the first 5 predictions on the test set, along with the actual labels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252db724",
   "metadata": {
    "id": "252db724"
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "linreg = LinearRegression()\n",
    "trained_model = linreg.train(X_train, y_train)\n",
    "y_train_pred = linreg.predict(X_train)\n",
    "y_test_pred = linreg.predict(X_test)\n",
    "y_val_pred = linreg.predict(X_val)\n",
    "\n",
    "mse_autompg_train = mean_squared_error(y_train,y_train_pred)\n",
    "mse_autompg_val = mean_squared_error(y_val,y_val_pred)\n",
    "mse_autompg_test = mean_squared_error(y_test,y_test_pred)\n",
    "\n",
    "print(\"MSE for train:\", mse_autompg_train)\n",
    "print(\"MSE for test:\", mse_autompg_test)\n",
    "print(\"MSE for val:\", mse_autompg_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad4d6f",
   "metadata": {
    "id": "9fad4d6f"
   },
   "source": [
    "**B) As a baseline model, use the mean of the training labels (auto_mpg_y_train) as the prediction for all instances. Report the mean-squared error (MSE) on the training, validation, and test sets using this baseline. This is a common baseline used in regression problems and tells you if your model is any good. Your linear regression MSEs should be much lower than these baseline MSEs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d948c5",
   "metadata": {
    "id": "74d948c5"
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "bse_y_train=[]\n",
    "bse_y_val=[]\n",
    "bse_y_test=[]\n",
    "\n",
    "for i in y_train_pred:\n",
    "    out = i - y_train\n",
    "    bse_y_train.append(out)\n",
    "bse_y_train_mean = np.square(bse_y_train).mean()\n",
    "\n",
    "for i in y_val_pred:\n",
    "    out = i - y_val\n",
    "    bse_y_val.append(out)\n",
    "bse_y_val_mean = np.square(bse_y_val).mean()\n",
    "\n",
    "for i in y_test_pred:\n",
    "    out = i - y_test\n",
    "    bse_y_test.append(out)\n",
    "bse_y_test_mean = np.square(bse_y_test).mean()\n",
    "\n",
    "print(\"Baseline MSE on y_train :\", bse_y_train_mean)\n",
    "print(\"Baseline MSE on y_val :\", bse_y_val_mean)\n",
    "print(\"Baseline MSE on y_test :\", bse_y_test_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f5bb6",
   "metadata": {
    "id": "726f5bb6"
   },
   "source": [
    "**1.12. Interpret your model trained on the auto MPG dataset using a bar chart of the model weights. Make sure to label the bars (x-axis) and don't forget the bias term! Use lecture 3, slide 15 as a reference. According to your model, which features are the greatest contributors to the selling price**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fccabaa",
   "metadata": {
    "id": "9fccabaa"
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "##cite : https://seaborn.pydata.org/examples/part_whole_bars.html\n",
    "\n",
    "values = X_train.columns.values\n",
    "fig,ax = plt.subplots(figsize=(60,50))\n",
    "ax = sns.barplot(y=trained_model, x=values,)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4d27e",
   "metadata": {},
   "source": [
    "**Ans** : From the plot above, as per mu understanding the best features are mileage, fuel_diesel, fuel_lpg and engine are the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12c284a",
   "metadata": {
    "id": "c12c284a"
   },
   "source": [
    "**Tune Regularization Parameter $\\alpha$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e73e6",
   "metadata": {
    "id": "2a3e73e6"
   },
   "source": [
    "**Now, let's do ridge regression and tune the $\\alpha$ regularization parameter on the auto MPG dataset.**\n",
    "\n",
    "**1.13. Sweep out values for $\\alpha$ using `alphas = np.logspace(-2, 1, 10)`. Perform a grid search over these $\\alpha$ values, recording the training and validation MSEs for each $\\alpha$. A simple grid search is fine, no need for k-fold cross validation. Plot the training and validation MSEs as a function of $\\alpha$ on a single figure. Make sure to label the axes and the training and validation MSE curves. Use a log scale for the x-axis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb6245",
   "metadata": {
    "id": "97cb6245"
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "###variables\n",
    "alphas = np.logspace(-2, 1, 10) #as mentioned\n",
    "mse_train=[]\n",
    "mse_val=[]\n",
    "for i in alphas:\n",
    "    linreg_new = LinearRegression(i)\n",
    "    model = linreg_new.train(X_train,y_train)\n",
    "    y_train_pred = linreg.predict(X_train)\n",
    "    y_val_pred = linreg.predict(X_val)\n",
    "    mse_train.append(mean_squared_error(y_train,y_train_pred))\n",
    "    mse_val.append(mean_squared_error(y_val,y_val_pred))\n",
    "    \n",
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.plot(alphas,mse_train,label=\"training MSE\")\n",
    "plt.plot(alphas,mse_val,label=\"validation MSE\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"alphas\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc61366",
   "metadata": {
    "id": "bcc61366"
   },
   "source": [
    "**Explain your plot above. How do training and validation MSE behave with decreasing model complexity (increasing $\\alpha$)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13098501",
   "metadata": {
    "id": "170c66bd"
   },
   "source": [
    "### Your answer here\n",
    "\n",
    "Ideally, as the value of alpha increases the model complexity also increases, which results in the increase in the values of MSE for both training and testing. In this plot we are getting the best alpha, which is majorly because of the bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf60747",
   "metadata": {
    "id": "faf60747"
   },
   "source": [
    "**1.14. Using the $\\alpha$ which gave the best validation MSE above, train a model on the training set. Report the value of $\\alpha$ and its training, validation, and test MSE. This is the final tuned model which you would deploy in production.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8f5a1",
   "metadata": {
    "id": "4ec8f5a1"
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n",
    "mse_min = min(mse_val)\n",
    "best_alpha = alphas[mse_val.index(mse_min)]\n",
    "\n",
    "linreg_best = LinearRegression(best_alpha)\n",
    "trained_model = linreg_best.train(X_train, y_train)\n",
    "y_train_pred = linreg_best.predict(X_train)\n",
    "y_test_pred = linreg_best.predict(X_test)\n",
    "y_val_pred = linreg_best.predict(X_val)\n",
    "mse_autompg_train_best= mean_squared_error(y_train,y_train_pred)\n",
    "mse_autompg_val_best = mean_squared_error(y_val,y_val_pred)\n",
    "mse_autompg_test_best = mean_squared_error(y_test,y_test_pred)\n",
    "\n",
    "print(\"Best Alpha:\", best_alpha)\n",
    "print(\"Best MSE for train:\", mse_autompg_train_best)\n",
    "print(\"Best MSE for test:\", mse_autompg_test_best)\n",
    "print(\"Best MSE for val:\", mse_autompg_val_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d5e341",
   "metadata": {
    "id": "05d5e341"
   },
   "source": [
    "# **Part 2: Logistic Regression**\n",
    "\n",
    "**Gender Recognition by Voice and Speech Analysis**\n",
    "\n",
    "**This dataset is used to identify a voice as male or female, based upon acoustic properties of the voice and speech.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f585b079",
   "metadata": {
    "id": "f585b079"
   },
   "outputs": [],
   "source": [
    "voice_df = pd.read_csv(\"voice-classification.csv\")\n",
    "voice_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b513a2",
   "metadata": {
    "id": "f0b513a2"
   },
   "source": [
    "**Data - Checking Rows & Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7505761",
   "metadata": {
    "id": "c7505761"
   },
   "outputs": [],
   "source": [
    "#Number of Rows & Columns\n",
    "print(voice_df.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b947d",
   "metadata": {
    "id": "305b947d"
   },
   "source": [
    "**2.1 What is the probability of observing different  categories in the Label feature of the dataset?**\n",
    "\n",
    "This is mainly to check class imbalance in the dataset, and to apply different techniques to balance the dataset, which we will learn later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab9321",
   "metadata": {
    "id": "5fab9321"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "\n",
    "count = voice_df[\"label\"].value_counts()\n",
    "m =  count[0]\n",
    "f = count[1]\n",
    "prob_m = m/len(voice_df)\n",
    "prob_f = 1-prob_m\n",
    "print(\"Probability of Male voice :\", prob_m)\n",
    "print(\"Probability of Female voice :\", prob_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c549ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_df.info()\n",
    "print(\"Shape of dataset :\",voice_df.shape)\n",
    "feature_list = list(voice_df.columns)\n",
    "print(\"Original dataframe feature list :\",feature_list)\n",
    "print(\"Length of Original dataframe feature list :\",len(feature_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ae809",
   "metadata": {
    "id": "6b8ae809"
   },
   "source": [
    "**2.2 Plot the relationships between the label and the 20 numerical features using a small multiple of box plots. Make sure to label the axes. What useful information do this plot provide?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b532f",
   "metadata": {
    "id": "da9b532f"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "\n",
    "fig,ax = plt.subplots(5,4, figsize=(20,24), sharey=True)\n",
    "ax = ax.ravel()\n",
    "feature_list = feature_list[:20]\n",
    "# print(feature_list)\n",
    "for i in range(0,len(feature_list)):\n",
    "    sns.boxplot(x=voice_df[str(feature_list[i])],y=voice_df['label'],data=voice_df,ax=ax[i])\n",
    "    ax[i].set_title(\"Label vs \" + feature_list[i])\n",
    "plt.show()\n",
    "fig.suptitle('Relationship between the Label and 20 features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cdbd8c",
   "metadata": {
    "id": "c0cdbd8c"
   },
   "source": [
    "**2.3 Plot the correlation matrix, and check if there is high correlation between the given numerical features (Threshold >=0.9). If yes, drop those highly correlated features from the dataframe. Why is necessary to drop those columns before proceeding further?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdcb929",
   "metadata": {
    "id": "0fdcb929"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "vcorr = voice_df.corr().abs()  ##3get the absolute value for the correlation to figure out which blocks to drop\n",
    "sns.heatmap(vcorr,annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa78aa4c",
   "metadata": {},
   "source": [
    "we need to find the huighly correlated features, so we can remove features greater than 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e343ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### cite : https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on\n",
    "\n",
    "###taken from the stackoverflow code on how to drop highly correlated feature columns\n",
    "high = vcorr.where(np.triu(np.ones(vcorr.shape), k=1).astype(np.bool))\n",
    "correlated_cols = [column for column in high.columns if any(high[column] > 0.90)]\n",
    "voice_df.drop(correlated_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6bd740",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New Dataframe with dropped features :\",list(voice_df.columns))\n",
    "print(\"Length of new dataframe :\", len(list(voice_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1eb8c4",
   "metadata": {
    "id": "6e1eb8c4"
   },
   "source": [
    "**Separating Features & Y variable from the processed dataset**\n",
    "\n",
    "**Please note to replace the dataframe below with the new dataframe created after removing highly correlated features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697d57b",
   "metadata": {
    "id": "0697d57b"
   },
   "outputs": [],
   "source": [
    "# Split data into features and labels\n",
    "voice_X = voice_df.drop(columns=['label']) #replace \"voice_df1\" with your dataframe from 2.3 to make sure the code runs\n",
    "voice_y = voice_df['label']\n",
    "print(voice_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42edec62",
   "metadata": {
    "id": "42edec62"
   },
   "source": [
    "**2.4 Apply the following pre-processing steps:**\n",
    "\n",
    "1) Use OrdinalEncoding to encode the label in the dataset (male & female)\n",
    "\n",
    "2) Convert the label from a Pandas series to a Numpy (m x 1) vector. If you don't do this, it may cause problems when implementing the logistic regression model.\n",
    "\n",
    "3)Split the dataset into training (60%), validation (20%), and test (20%) sets.\n",
    "\n",
    "4) Standardize the columns in the feature matrices. To avoid information leakage, learn the standardization parameters from training, and then apply training, validation and test dataset.\n",
    "\n",
    "5) Add a column of ones to the feature matrices of train, validation and test dataset. This is a common trick so that we can learn a coefficient for the bias term of a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b06f8",
   "metadata": {
    "id": "a99b06f8"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "\n",
    "###ordinal encoding\n",
    "from sklearn.preprocessing import OrdinalEncoder   #for ordinal encoder\n",
    "encoded_label = OrdinalEncoder(categories=[[\"male\",\"female\"]])\n",
    "voice_y = voice_y.to_numpy().reshape(-1,1)\n",
    "voice_y = encoded_label.fit_transform(voice_y)\n",
    "\n",
    "###split dataset\n",
    "X_dev_v, X_test_v, y_dev_v, y_test_v = train_test_split(voice_X, voice_y, test_size=0.2, shuffle=True)\n",
    "X_train_v, X_val_v, y_train_v, y_val_v = train_test_split(X_dev_v, y_dev_v, test_size=0.25, shuffle=True)\n",
    "print(\"X_train shape:\", X_train_v.shape)\n",
    "print(\"X_test shape:\", X_test_v.shape)\n",
    "print(\"X_val shape:\", X_val_v.shape)\n",
    "\n",
    "###standardize the dataset \n",
    "###cite : https://stackoverflow.com/questions/24645153/pandas-dataframe-columns-scaling-with-sklearn\n",
    "t1 = X_train_v.columns\n",
    "t2 = X_test_v.columns\n",
    "t3 = X_val_v.columns\n",
    "X_train_v[t1] = StandardScaler().fit_transform(X_train_v[t1])\n",
    "X_test_v[t2] = StandardScaler().fit_transform(X_test_v[t2])\n",
    "X_val_v[t3] = StandardScaler().fit_transform(X_val_v[t3])\n",
    "print(\"testing standardizing on X_train:\", X_train_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63db53",
   "metadata": {
    "id": "ff63db53"
   },
   "source": [
    "**2.5 Implement Logistic Regression**\n",
    "\n",
    "We will now implement logistic regression with L2 regularization. Given an (m x n) feature matrix $X$, an (m x 1) label vector $y$, and an (n x 1) weight vector $w$, the hypothesis function for logistic regression is:\n",
    "\n",
    "$$\n",
    "y = \\sigma(X w)\n",
    "$$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$, i.e. the sigmoid function. This function scales the prediction to be a probability between 0 and 1, and can then be thresholded to get a discrete class prediction.\n",
    "\n",
    "Just as with linear regression, our objective in logistic regression is to learn the weights $𝑤$ which best fit the data. For L2-regularized logistic regression, we find an optimal $w$ to minimize the following loss function:\n",
    "\n",
    "$$\n",
    "\\min_{w} \\ -y^T \\ \\text{log}(\\sigma(Xw)) \\ - \\  (\\mathbf{1} - y)^T \\ \\text{log}(\\mathbf{1} - \\sigma(Xw)) \\ + \\ \\alpha \\| w \\|^2_2 \\\\\n",
    "$$\n",
    "\n",
    "Unlike linear regression, however, logistic regression has no closed-form solution for the optimal $w$. So, we will use gradient descent to find the optimal $w$. The (n x 1) gradient vector $g$ for the loss function above is:\n",
    "\n",
    "$$\n",
    "g = X^T \\Big(\\sigma(Xw) - y\\Big) + 2 \\alpha w\n",
    "$$\n",
    "\n",
    "Below is pseudocode for gradient descent to find the optimal $w$. You should first initialize $w$ (e.g. to a (n x 1) zero vector). Then, for some number of epochs $t$, you should update $w$ with $w - \\eta g $, where $\\eta$ is the learning rate and $g$ is the gradient. You can learn more about gradient descent [here](https://www.coursera.org/lecture/machine-learning/gradient-descent-8SpIM).\n",
    "\n",
    "> $w = \\mathbf{0}$\n",
    "> \n",
    "> $\\text{for } i = 1, 2, ..., t$\n",
    ">\n",
    "> $\\quad \\quad w = w - \\eta g $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b33ad",
   "metadata": {
    "id": "809b33ad"
   },
   "source": [
    "Implement a LogisticRegression class with five methods: train, predict, calculate_loss, calculate_gradient, and calculate_sigmoid. **You may NOT use sklearn for this implementation. It is highly recommended that you vectorize your code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcee7c",
   "metadata": {
    "id": "0cdcee7c"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    '''\n",
    "    Logistic regression model with L2 regularization.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    alpha: regularization parameter\n",
    "    t: number of epochs to run gradient descent\n",
    "    eta: learning rate for gradient descent\n",
    "    w: (n x 1) weight vector\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, alpha, t, eta):\n",
    "        self.alpha = alpha\n",
    "        self.t = t\n",
    "        self.eta = eta\n",
    "        self.w = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        '''Trains logistic regression model using gradient descent \n",
    "        (sets w to its optimal value).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        y: (m x 1) label vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        losses: (t x 1) vector of losses at each epoch of gradient descent\n",
    "        '''\n",
    "        ### Your code here\n",
    "        \n",
    "        n = X.shape[1]\n",
    "        final_loss = []\n",
    "        self.w = np.random.rand(n,1)\n",
    "#         print(self.w.shape)\n",
    "        for i in range(self.t):\n",
    "            self.w = self.w - (self.eta * self.calculate_gradient(X,y))\n",
    "            final_loss.append(self.calculate_loss(X,y))\n",
    "        losses = final_loss\n",
    "\n",
    "        return losses\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''Predicts on X using trained model. Make sure to threshold \n",
    "        the predicted probability to return a 0 or 1 prediction.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: (m x 1) 0/1 prediction vector\n",
    "        '''\n",
    "        ### Your code here\n",
    "\n",
    "        a = self.calculate_sigmoid(np.dot(X,self.w))\n",
    "        a[a > 0.5] = 1\n",
    "        a[a < 0.5] = 0\n",
    "        return a\n",
    "    \n",
    "    def calculate_loss(self, X, y):\n",
    "        '''Calculates the logistic regression loss using X, y, w, \n",
    "        and alpha. Useful as a helper function for train().\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        y: (m x 1) label vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss: (scalar) logistic regression loss\n",
    "        '''\n",
    "        ### Your code here\n",
    "        \n",
    "        m = X.shape[0] ###divide with the total \n",
    "        a = self.calculate_sigmoid(np.dot(X,self.w))\n",
    "        \n",
    "        ###for getting loss across the entire length of data, it is normalized when divided by m\n",
    "        ##hence removing it for the instructions of getting a convergence curve\n",
    "#       loss = (- 1 / m) * np.sum(y * np.log(a) + (1 - y) * (np.log(1 - a))) + 0.5 * (1 / m) *  self.alpha * np.sum(self.w * self.w)\n",
    "        \n",
    "        loss = (- 1 ) * np.sum(y * np.log(a) + (1 - y) * (np.log(1 - a))) + 0.5 * (1) *  self.alpha * np.sum(self.w * self.w)\n",
    "        return loss\n",
    "    \n",
    "    def calculate_gradient(self, X, y):\n",
    "        '''Calculates the gradient of the logistic regression loss \n",
    "        using X, y, w, and alpha. Useful as a helper function \n",
    "        for train().\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        y: (m x 1) label vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        gradient: (n x 1) gradient vector for logistic regression loss\n",
    "        '''\n",
    "        ### Your code here\n",
    "        m = X.shape[0]\n",
    "        a = self.calculate_sigmoid(np.dot(X,self.w)) ##activation\n",
    "        \n",
    "        ###for getting gradient the entire length of dats, it is normalized when divided by m\n",
    "        ##hence removing it for instructions of getting a convergence curve\n",
    "#       J = (1 / m) * np.dot(X.T, (a - y)) + 2 * self.alpha * self.w\n",
    "\n",
    "        J = (1) * np.dot(X.T, (a - y)) + 2 * self.alpha * self.w\n",
    "        return J\n",
    "        \n",
    "    \n",
    "    def calculate_sigmoid(self, x):\n",
    "        '''Calculates the sigmoid function on each element in vector x. \n",
    "        Useful as a helper function for predict(), calculate_loss(), \n",
    "        and calculate_gradient().\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: (m x 1) vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        sigmoid_x: (m x 1) vector of sigmoid on each element in x\n",
    "        '''\n",
    "        ### Your code here\n",
    "        \n",
    "        h = np.zeros_like(x)\n",
    "        h = (1/(1+np.exp(-x)))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f34a6f",
   "metadata": {
    "id": "d6f34a6f"
   },
   "source": [
    "**2.6 Plot Loss over Epoch and Search the space randomly to find best hyperparameters.**\n",
    "\n",
    "A: Using your implementation above, train a logistic regression model **(alpha=0, t=100, eta=1e-3)** on the voice recognition training data. Plot the training loss over epochs. Make sure to label your axes. You should see the loss decreasing and start to converge. \n",
    "\n",
    "B: Using **alpha between (0,1), eta between(0, 0.001) and t between (0, 100)**, find the best hyperparameters for LogisticRegression. You can randomly search the space 20 times to find the best hyperparameters.\n",
    "\n",
    "C. Compare accuracy on the test dataset for both the scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785b3bb",
   "metadata": {
    "id": "c785b3bb"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "\n",
    "logr = LogisticRegression(alpha=0,t=100,eta=1e-3)\n",
    "loss = logr.train(X_train_v, y_train_v)\n",
    "y_pred = logr.predict(X_test_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27be426",
   "metadata": {},
   "source": [
    "### a). Plotting logistic regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eeb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = range(0,100)\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "ax = sns.lineplot()\n",
    "ax = sns.lineplot(x=t,y=loss)\n",
    "ax.set_xlabel(\"epochs\")\n",
    "ax.set_ylabel(\"training loss\")\n",
    "plt.title(\"loss curve over epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca1558",
   "metadata": {},
   "source": [
    "### b). find the best hyperparameters for LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da196ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss=[]\n",
    "best_parameters=[]\n",
    "alpha = np.random.uniform(0, 1)\n",
    "eta = np.random.uniform(0, 0.001)\n",
    "t = np.random.randint(0,100)\n",
    "\n",
    "##search random space\n",
    "for i in range(20):\n",
    "    random_lr = LogisticRegression(alpha,t,eta)\n",
    "    random_loss = random_lr.train(X_train_v,y_train_v)\n",
    "    best_loss.append(random_loss)\n",
    "    best_parameters.append([alpha,eta,t])\n",
    "    \n",
    "best_loss_np = np.array(best_loss)\n",
    "best_loss_idx = np.where(best_loss_np == best_loss_np.min())\n",
    "best_t = best_parameters[best_loss_idx[0][0]][2]\n",
    "best_eta = best_parameters[best_loss_idx[0][0]][1]\n",
    "best_alpha_lr = best_parameters[best_loss_idx[0][0]][0]\n",
    "\n",
    "print(\"Best Alpha:\", best_alpha_lr)\n",
    "print(\"Best ETA:\", best_eta)\n",
    "print(\"Best t:\", best_t )\n",
    "print(\"Best loss:\", best_loss_np.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f188a581",
   "metadata": {
    "id": "f188a581"
   },
   "source": [
    "**2.7 Feature Importance**\n",
    "\n",
    "Interpret your trained model using a bar chart of the model weights. Make sure to label the bars (x-axis) and don't forget the bias term! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31e859",
   "metadata": {
    "id": "8b31e859"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "\n",
    "print(\"Best accuracy for original:\", accuracy_score(y_test_v,y_pred))\n",
    "\n",
    "best_logr = LogisticRegression(best_alpha_lr,best_t,best_eta)\n",
    "best_loss_new = best_logr.train(X_train_v,y_train_v)\n",
    "y_pred_best = best_logr.predict(X_test_v)\n",
    "print(\"Best accuracy for best parameteres chosen model:\", accuracy_score(y_test_v,y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3307a991",
   "metadata": {
    "id": "3307a991"
   },
   "source": [
    "\n",
    "# **Part 3: Support Vector Machines - with the same Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_cdNyA8r8eNX",
   "metadata": {
    "id": "_cdNyA8r8eNX"
   },
   "source": [
    "**3.1 Dual SVM**\n",
    "\n",
    "A) Train a dual SVM (with default parameters) for both kernel=“linear” and kernel=“rbf”) on the Voice Recognition training data.\n",
    "\n",
    "B) Make predictions and report the accuracy on the training, validation, and test sets. Which kernel gave better accuracy on test dataset and why do you think that was better?\n",
    "\n",
    "C) Please report the support vectors in both the cases and what do you observe? Explain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zq6AK8Jt8gWN",
   "metadata": {
    "id": "zq6AK8Jt8gWN"
   },
   "outputs": [],
   "source": [
    "#code here \n",
    "\n",
    "##a&b completed\n",
    "\n",
    "## cite : https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "###model linear\n",
    "print(\"Linear Model Statistics\")\n",
    "model_linear = SVC(kernel = 'linear')\n",
    "model_linear = model_linear.fit(X_train_v,y_train_v)\n",
    "print(\"Model Score:\", model_linear.score(X_train_v,y_train_v))\n",
    "predicted_train=model_linear.predict(X_train_v)\n",
    "predicted_test=model_linear.predict(X_test_v)\n",
    "predicted_val=model_linear.predict(X_val_v)\n",
    "acc_train = accuracy_score(y_train_v,predicted_train)\n",
    "acc_test = accuracy_score(y_test_v,predicted_test)\n",
    "acc_val = accuracy_score(y_val_v,predicted_val)\n",
    "print(\"Training Accuracy for the Linear Model :\", acc_train)\n",
    "print(\"Testing Accuracy for the Linear Model :\", acc_test)\n",
    "print(\"Validation Accuracy for the Linear Model :\", acc_val)\n",
    "svc_linear = model_linear.support_vectors_\n",
    "\n",
    "print(\"-\"*70)\n",
    "\n",
    "###model linear\n",
    "print(\"rbf Model Statistics\")\n",
    "model_rbf = SVC(kernel = 'rbf')\n",
    "model_rbf = model_rbf.fit(X_train_v,y_train_v)\n",
    "print(\"Model Score:\", model_rbf.score(X_train_v,y_train_v))\n",
    "predicted_train=model_rbf.predict(X_train_v)\n",
    "predicted_test=model_rbf.predict(X_test_v)\n",
    "predicted_val=model_rbf.predict(X_val_v)\n",
    "acc_train_rbf = accuracy_score(y_train_v,predicted_train)\n",
    "acc_test_rbf = accuracy_score(y_test_v,predicted_test)\n",
    "acc_val_rbf = accuracy_score(y_val_v,predicted_val)\n",
    "print(\"Training Accuracy for the rbf Model :\", acc_train_rbf)\n",
    "print(\"Testing Accuracy for the rbf Model :\", acc_test_rbf)\n",
    "print(\"Validation Accuracy for the rbd Model :\", acc_val_rbf)\n",
    "svc_rbf = model_rbf.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7899cd",
   "metadata": {},
   "source": [
    "#### c (a&b in previous cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95177fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SV for linear', svc_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SV for RBF', svc_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcf41e5",
   "metadata": {},
   "source": [
    "**Ans :** As we can see from the above calculations and model training, SVM for RBF kernel performs better than the Linear SVM. One of the possible reasons which might cause this behaviour is because of the fact that data might not not linearly separable. Usually, Linear SVM is used when the data is linearly separable. in this case, we could have also used polynomial svm as well. To test we can also try various values of gamma and cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P2h1rkaQ8rOr",
   "metadata": {
    "id": "P2h1rkaQ8rOr"
   },
   "source": [
    "**3.2 Using Kernel “rbf”, tune the hyperparameter “C” using the Grid Search & k-fold cross validation. You may take k=5 and assume values in grid between 1 to 100 with interval range of your choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QVoqZtQ38wPN",
   "metadata": {
    "id": "QVoqZtQ38wPN"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "\n",
    "###cite : geeks for geeks : https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/\n",
    "\n",
    "# print(np.linspace(1, 100, num=50))\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': np.linspace(1, 100, num=20),\n",
    "              'kernel': ['rbf']}\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=5, refit = True, verbose = 3)\n",
    "grid.fit(X_dev_v,y_dev_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56530d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "71ebca87",
    "1e3b7ca8",
    "c48a741e",
    "c12c284a"
   ],
   "name": "Assignment_1_Spring_22_updated.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
